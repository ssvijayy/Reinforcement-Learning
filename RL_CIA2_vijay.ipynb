{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIA-2 assignment : #\n",
    "Create a 100x100 grid with obstacles in between 2 random points. Build an MDP based RL agent to optimise both policies and actions at every state. Benchmark DP method with other RL solutions for the same problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By:\n",
    "Vijay Shrinivas Senthilnathan\n",
    "Reg.no: 21011101143\n",
    "R.no: 21110094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings\n",
    "GRID_SIZE = 100\n",
    "OBSTACLE_DENSITY = 0.2  # Percentage of cells that are obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
    "ACTION_NAMES = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size, obstacle_density=0.2):\n",
    "        self.size = size\n",
    "        self.obstacle_density = obstacle_density\n",
    "        self.grid = np.zeros((size, size))\n",
    "        self.start, self.goal = self._place_start_and_goal()\n",
    "        self.obstacles = self._place_obstacles()  # Store obstacle positions\n",
    "\n",
    "    def _place_start_and_goal(self):\n",
    "        start = (random.randint(0, self.size - 1), random.randint(0, self.size - 1))\n",
    "        goal = (random.randint(0, self.size - 1), random.randint(0, self.size - 1))\n",
    "        while goal == start:\n",
    "            goal = (random.randint(0, self.size - 1), random.randint(0, self.size - 1))\n",
    "        return start, goal\n",
    "\n",
    "    def _place_obstacles(self):\n",
    "        obstacles = []\n",
    "        num_obstacles = int(self.size * self.size * self.obstacle_density)\n",
    "        while len(obstacles) < num_obstacles:\n",
    "            x, y = random.randint(0, self.size - 1), random.randint(0, self.size - 1)\n",
    "            if (x, y) != self.start and (x, y) != self.goal and (x, y) not in obstacles:\n",
    "                obstacles.append((x, y))\n",
    "                self.grid[x, y] = -1  # -1 indicates an obstacle\n",
    "        return obstacles\n",
    "\n",
    "    def is_obstacle(self, state):\n",
    "        x, y = state\n",
    "        return self.grid[x, y] == -1\n",
    "\n",
    "    def step(self, state, action):\n",
    "        if self.is_obstacle(state):\n",
    "            return state, -10  # -10 reward for hitting an obstacle\n",
    "\n",
    "        x, y = state\n",
    "        if action == UP:\n",
    "            next_state = (max(0, x - 1), y)\n",
    "        elif action == DOWN:\n",
    "            next_state = (min(self.size - 1, x + 1), y)\n",
    "        elif action == LEFT:\n",
    "            next_state = (x, max(0, y - 1))\n",
    "        elif action == RIGHT:\n",
    "            next_state = (x, min(self.size - 1, y + 1))\n",
    "\n",
    "        reward = -1  # Cost for each step\n",
    "        if next_state == self.goal:\n",
    "            reward = 100  # Reward for reaching the goal\n",
    "        return next_state, reward\n",
    "\n",
    "    def reset(self):\n",
    "        return self.start  # Reset to the starting position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid World Representation:\n",
      "[['X' 'X' 'X' 'O' 'O' 'O' 'O' 'O' 'O' 'X']\n",
      " ['O' 'O' 'O' 'X' 'O' 'O' 'O' 'X' 'O' 'X']\n",
      " ['O' 'O' 'S' 'O' 'O' 'O' 'X' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'X' 'X' 'O' 'O' 'O' 'O' 'X' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'X' 'O' 'X' 'O' 'O' 'X']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['G' 'O' 'O' 'X' 'X' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'X' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'X' 'O' 'X']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_grid(env):\n",
    "    # Create a matrix representation of the grid\n",
    "    grid_matrix = np.full((env.size, env.size), 'O')  # Fill with free space\n",
    "\n",
    "    # Add obstacles\n",
    "    for obstacle in env.obstacles:\n",
    "        grid_matrix[obstacle[0], obstacle[1]] = 'X'  # Set obstacle\n",
    "\n",
    "    # Set start position\n",
    "    grid_matrix[env.start[0], env.start[1]] = 'S'  # Set start\n",
    "\n",
    "    # Set goal position\n",
    "    grid_matrix[env.goal[0], env.goal[1]] = 'G'  # Set goal\n",
    "\n",
    "    # Print the matrix\n",
    "    print(\"Grid World Representation:\")\n",
    "    print(grid_matrix)\n",
    "\n",
    "    # Optional: Visual representation using Matplotlib\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid_matrix == 'X', cmap='gray', vmin=0, vmax=1)  # Show obstacles\n",
    "    plt.scatter(env.start[1], env.start[0], marker='o', color='blue', label='Start (S)', s=100)\n",
    "    plt.scatter(env.goal[1], env.goal[0], marker='o', color='green', label='Goal (G)', s=100)\n",
    "    plt.title(\"Grid World\")\n",
    "    plt.xticks(range(env.size))\n",
    "    plt.yticks(range(env.size))\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match matrix representation\n",
    "    plt.grid(False)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage of GridWorld\n",
    "GRID_SIZE = 10  # Adjust as needed\n",
    "OBSTACLE_DENSITY = 0.2  # 20% obstacles\n",
    "env = GridWorld(GRID_SIZE, OBSTACLE_DENSITY)\n",
    "\n",
    "# Call the visualize function\n",
    "visualize_grid(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP Value Iteration\n",
    "class MDP:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.value_table = np.zeros((env.size, env.size))\n",
    "        self.policy_table = np.zeros((env.size, env.size), dtype=int)\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "\n",
    "    def value_iteration(self, max_iterations=1000, epsilon=1e-4):\n",
    "        for i in range(max_iterations):\n",
    "            delta = 0\n",
    "            for x in range(self.env.size):\n",
    "                for y in range(self.env.size):\n",
    "                    if (x, y) == self.env.goal or self.env.is_obstacle((x, y)):\n",
    "                        continue\n",
    "                    v = self.value_table[x, y]\n",
    "                    self.value_table[x, y] = max(\n",
    "                        [sum([self.transition_probability((x, y), action) *\n",
    "                              (reward + self.gamma * self.value_table[next_x, next_y])\n",
    "                              for next_x, next_y, reward in self.transitions((x, y), action)])\n",
    "                         for action in ACTIONS]\n",
    "                    )\n",
    "                    delta = max(delta, abs(v - self.value_table[x, y]))\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "        self.update_policy()\n",
    "\n",
    "    def transition_probability(self, state, action):\n",
    "        return 1.0  # Deterministic environment\n",
    "\n",
    "    def transitions(self, state, action):\n",
    "        next_state, reward = self.env.step(state, action)\n",
    "        return [(next_state[0], next_state[1], reward)]\n",
    "\n",
    "    def update_policy(self):\n",
    "        for x in range(self.env.size):\n",
    "            for y in range(self.env.size):\n",
    "                if (x, y) == self.env.goal or self.env.is_obstacle((x, y)):\n",
    "                    continue\n",
    "                best_action = np.argmax([\n",
    "                    sum([self.transition_probability((x, y), action) *\n",
    "                         (reward + self.gamma * self.value_table[next_x, next_y])\n",
    "                         for next_x, next_y, reward in self.transitions((x, y), action)])\n",
    "                    for action in ACTIONS\n",
    "                ])\n",
    "                self.policy_table[x, y] = best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyBandit:\n",
    "    def __init__(self, env, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.q_table = np.zeros((env.size, env.size, len(ACTIONS)))\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            steps = 0\n",
    "            while state != self.env.goal:\n",
    "                if random.random() < self.epsilon:\n",
    "                    action = random.choice(ACTIONS)\n",
    "                else:\n",
    "                    action = np.argmax(self.q_table[state[0], state[1], :])\n",
    "                \n",
    "                next_state, reward = self.env.step(state, action)\n",
    "                best_next_action = np.argmax(self.q_table[next_state[0], next_state[1], :])\n",
    "                \n",
    "                # Update Q-table using the Q-learning formula\n",
    "                self.q_table[state[0], state[1], action] += 0.1 * (\n",
    "                    reward + self.gamma * self.q_table[next_state[0], next_state[1], best_next_action]\n",
    "                    - self.q_table[state[0], state[1], action]\n",
    "                )\n",
    "\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "\n",
    "                if steps > 200:  # Cap steps to prevent excessively long episodes\n",
    "                    break\n",
    "\n",
    "            # Decay epsilon to shift from exploration to exploitation\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, env, n_actions=len(ACTIONS), epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.n_actions = n_actions\n",
    "        self.q_values = np.zeros(n_actions)\n",
    "        self.action_counts = np.zeros(n_actions)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(range(self.n_actions))\n",
    "        return np.argmax(self.q_values)\n",
    "\n",
    "    def train(self, episodes=500):  # Reduced episodes for quicker testing\n",
    "        for _ in range(episodes):\n",
    "            action = self.select_action()  # Select an action\n",
    "            state = self.env.reset()\n",
    "            steps = 0\n",
    "            while state != self.env.goal and steps < 100:  # Limit steps per episode\n",
    "                next_state, reward = self.env.step(state, action)\n",
    "                self.action_counts[action] += 1\n",
    "                # Incremental average update for Q-value\n",
    "                self.q_values[action] += (reward - self.q_values[action]) / self.action_counts[action]\n",
    "                state = next_state\n",
    "                steps += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "    def __init__(self, env, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.q_table = np.zeros((env.size, env.size, len(ACTIONS)))\n",
    "        self.returns = [[{} for _ in range(env.size)] for _ in range(env.size)]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def train(self, episodes=500):  # Reduced episodes for quicker testing\n",
    "        for _ in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            episode = []\n",
    "            steps = 0\n",
    "            \n",
    "            # Generate an episode using epsilon-greedy policy\n",
    "            while state != self.env.goal and steps < 100:  # Limit episode length\n",
    "                if random.random() < self.epsilon:\n",
    "                    action = random.choice(ACTIONS)\n",
    "                else:\n",
    "                    action = np.argmax(self.q_table[state[0], state[1], :])\n",
    "                \n",
    "                next_state, reward = self.env.step(state, action)\n",
    "                episode.append((state, action, reward))\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                if steps >= 200:  # Break if too many steps taken\n",
    "                    break\n",
    "            \n",
    "            # Calculate returns and update Q-table\n",
    "            G = 0\n",
    "            visited = set()\n",
    "            for t in reversed(range(len(episode))):\n",
    "                state, action, reward = episode[t]\n",
    "                G = reward + self.gamma * G\n",
    "                if (state, action) not in visited:\n",
    "                    visited.add((state, action))\n",
    "                    x, y = state\n",
    "                    if action not in self.returns[x][y]:\n",
    "                        self.returns[x][y][action] = []\n",
    "                    self.returns[x][y][action].append(G)\n",
    "                    self.q_table[x, y, action] = np.mean(self.returns[x][y][action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment and Agents\n",
    "env = GridWorld(GRID_SIZE, OBSTACLE_DENSITY)\n",
    "mdp_agent = MDP(env)\n",
    "bandit_agent = EpsilonGreedyBandit(env)\n",
    "multi_arm_bandit_agent = MultiArmedBandit(env)\n",
    "monte_carlo_agent = MonteCarlo(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Policy Table:\n",
      "[[1 0 0 1 1 0 3 1 1 0]\n",
      " [0 3 3 3 0 2 0 1 1 1]\n",
      " [0 0 0 0 0 0 2 2 2 2]\n",
      " [3 0 3 3 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 0]\n",
      " [0 0 3 3 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 2 0 2 0 0 0]\n",
      " [3 0 0 0 0 0 0 0 2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Run MDP Value Iteration\n",
    "mdp_agent.value_iteration()\n",
    "print(\"MDP Policy Table:\")\n",
    "print(mdp_agent.policy_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-Greedy Bandit Q-Table:\n",
      "[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-3.57480613e+01 -3.58897948e+01 -3.54365082e+01 -3.56142845e+01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-3.71543392e+01 -3.62941756e+01 -3.64524202e+01 -3.69027804e+01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-3.33048121e+01 -3.35672119e+01 -3.36942348e+01 -3.33947938e+01]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-1.00000000e-01 -1.09000000e-01  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-1.00000000e-01 -1.00000000e-01  0.00000000e+00  0.00000000e+00]\n",
      "  [-5.84895402e+01 -5.81595068e+01 -5.82932449e+01 -5.79981238e+01]\n",
      "  [-1.00000000e-01 -1.00000000e-01 -1.00000000e-01  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [-1.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[-8.87557966e+01 -8.87914158e+01 -8.87552509e+01 -8.88394251e+01]\n",
      "  [-1.90000000e-01 -1.17100000e-01 -6.59262633e+00 -5.48518861e+00]\n",
      "  [-7.36242806e+01 -7.36634009e+01 -7.38075282e+01 -7.37871410e+01]\n",
      "  [-9.99656846e+01 -9.99656566e+01 -9.99658119e+01 -9.99658967e+01]\n",
      "  [ 1.00000000e+02  7.00030206e+01 -6.92251318e+01  5.60586945e+01]\n",
      "  [-1.90000000e-01  4.01884412e-01  8.20133805e+01 -1.00000000e-01]\n",
      "  [-2.99886523e+00 -1.00000000e-01 -1.00000000e-01 -1.00000000e-01]\n",
      "  [-1.90000000e-01 -7.50851536e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00 -1.00000000e-01  0.00000000e+00]\n",
      "  [-1.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[-1.48217404e+01 -6.52903510e-01 -5.77339084e-01 -4.90234526e-01]\n",
      "  [-4.18510000e-01 -4.67223387e-01 -5.00889645e-01 -4.27492900e-01]\n",
      "  [-3.21396345e+00 -9.08441446e+00 -2.37070000e-01  8.12324209e+00]\n",
      "  [-3.01774392e+00 -1.72458667e+01 -1.33448445e-01  7.53231521e+01]\n",
      "  [ 8.90000000e+01  6.60244625e+01  5.59626099e+01  4.03872656e+01]\n",
      "  [-3.43900000e-01 -4.51420459e-01  6.79493529e+01 -3.69829000e-01]\n",
      "  [-2.71000000e-01 -2.15200000e-01 -2.30590000e-01 -7.79487945e+00]\n",
      "  [-9.29091396e+01 -9.28923397e+01 -9.28656398e+01 -9.28892099e+01]\n",
      "  [-1.00000000e-01 -1.09000000e-01 -8.09183104e+00  0.00000000e+00]\n",
      "  [-1.00000000e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[-7.76693173e-01 -8.73289174e-01 -7.72553056e-01 -8.61666619e-01]\n",
      "  [-8.05399725e-01 -9.67743341e-01 -7.53042510e-01 -5.15976020e+01]\n",
      "  [-9.99999984e+01 -9.99999984e+01 -9.99999984e+01 -9.99999984e+01]\n",
      "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 -1.00000000e+02]\n",
      "  [ 7.91000000e+01  5.40905718e+01 -8.73805623e+01  4.69615844e+01]\n",
      "  [ 6.98006363e-01  5.22269299e+00  6.58027767e+01 -5.42894410e-01]\n",
      "  [-4.20049000e-01 -5.06071466e-01  2.60012417e+00 -4.45258784e-01]\n",
      "  [-6.58503715e+00 -4.22468960e-01 -4.11807088e-01 -3.43900000e-01]\n",
      "  [-2.71000000e-01 -1.99000000e-01 -3.25360000e-01 -1.90000000e-01]\n",
      "  [-1.00000000e-01 -1.00000000e-01 -1.17100000e-01  0.00000000e+00]]\n",
      "\n",
      " [[-1.03548007e+00 -1.03310257e+00 -1.04642153e+00  4.89586762e+00]\n",
      "  [-1.32017068e+00 -5.17656036e+01 -1.16028300e+00  4.60691720e+01]\n",
      "  [-8.50502948e+01  3.35307802e+01  3.35911921e+01  5.49539000e+01]\n",
      "  [-8.14844263e+01  4.23929043e+01  3.97269283e+01  6.21710000e+01]\n",
      "  [ 7.01900000e+01  3.60236929e+01  4.40175490e+01  3.84128175e+01]\n",
      "  [ 7.44309719e+00 -1.02706965e+00  5.80769295e+01 -9.59588829e-01]\n",
      "  [-6.73306437e-01 -8.28441212e-01 -6.97504255e-01 -6.64587203e-01]\n",
      "  [-4.63513600e-01 -5.12094517e-01 -4.50135065e-01 -4.26610000e-01]\n",
      "  [-3.61000000e-01 -9.52027079e+00 -1.09882900e-01 -1.00000000e-01]\n",
      "  [-1.00000000e-01 -1.00000000e-01 -1.00000000e-01 -1.00000000e-01]]\n",
      "\n",
      " [[-8.47467643e-01 -1.71242852e+01 -7.72553056e-01 -2.46609239e+01]\n",
      "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 -1.00000000e+02]\n",
      "  [ 4.84585100e+01  2.88288891e+01 -9.01959220e+01  3.82651138e+01]\n",
      "  [ 8.84365931e+00 -2.08950003e+00 -7.53758110e-02  5.47266245e+01]\n",
      "  [ 6.21402340e+01 -6.48711160e+01  8.58832431e+00 -2.44629443e-01]\n",
      "  [ 1.15883857e+01 -1.19756917e+00 -6.30694521e-01 -1.10102987e+00]\n",
      "  [-8.26497765e-01 -2.81239677e+01 -7.53863032e-01 -7.89867864e-01]\n",
      "  [-5.55943510e-01 -5.78365217e-01 -6.92241956e-01 -1.21205800e+01]\n",
      "  [-9.48201994e+01 -9.48272989e+01 -9.48410431e+01 -9.48289009e+01]\n",
      "  [-1.00000000e-01 -1.00000000e-01 -8.37557937e+00  0.00000000e+00]]\n",
      "\n",
      " [[-9.99318025e+01 -9.99313516e+01 -9.99311362e+01 -9.99314656e+01]\n",
      "  [-6.51924599e+01 -2.92248562e+00 -6.20829845e+01  2.81339241e+01]\n",
      "  [ 4.26126590e+01  2.04170472e+01  1.15568905e+01  1.70236889e+01]\n",
      "  [ 3.64799050e+01 -2.62042219e+00 -1.37298398e+00 -8.07717083e+01]\n",
      "  [-1.00000000e+02 -1.00000000e+02 -1.00000000e+02 -1.00000000e+02]\n",
      "  [-1.36040173e+00 -1.41208566e+00 -4.26063845e+01 -3.55786692e+01]\n",
      "  [-9.99697404e+01 -9.99698380e+01 -9.99696184e+01 -9.99695710e+01]\n",
      "  [-5.83465943e-01 -7.89287140e-01 -2.32386242e+01 -6.34525263e-01]\n",
      "  [-7.99764831e+00 -7.38384980e+00 -3.64043791e-01 -3.61000000e-01]\n",
      "  [-1.90000000e-01 -7.35563551e+00 -1.11333610e-01 -1.90000000e-01]]\n",
      "\n",
      " [[-9.99987269e+01 -9.99987356e+01 -9.99987164e+01 -9.99987253e+01]\n",
      "  [-2.89237013e+00 -2.79797727e+00 -7.19963899e+01 -2.75209480e+00]\n",
      "  [ 3.33527528e+01 -2.82232817e+00 -2.95477189e+00 -2.78957822e+00]\n",
      "  [-2.47507444e+00 -2.46294621e+00 -1.10308199e+00 -2.42530191e+00]\n",
      "  [-7.01554689e+01 -2.04444076e+00 -2.11324501e+00 -1.99794168e+00]\n",
      "  [-1.65055706e+00 -1.56055199e+00 -1.77027252e+00 -1.59319158e+00]\n",
      "  [-3.68484947e+01 -1.33691420e+00 -1.35324612e+00 -1.36925112e+00]\n",
      "  [-1.04110599e+00 -1.05329930e+00 -1.09590184e+00 -3.12986803e+01]\n",
      "  [-9.76757568e+01 -9.76476655e+01 -9.76660349e+01 -9.76773647e+01]\n",
      "  [-8.62255006e+01 -8.63286085e+01 -8.62681918e+01 -8.63153223e+01]]\n",
      "\n",
      " [[-4.71920198e+01 -2.75207244e+00 -2.73926122e+00 -2.77575681e+00]\n",
      "  [-2.76989182e+00 -2.72828470e+00 -2.75235449e+00 -2.70672931e+00]\n",
      "  [-2.70132971e+00 -2.67507236e+00 -2.63055785e+00 -2.62341346e+00]\n",
      "  [-2.36914252e+00 -2.38462891e+00 -2.40928460e+00 -2.39678091e+00]\n",
      "  [-2.05119928e+00 -2.05276807e+00 -2.10839558e+00 -2.05841236e+00]\n",
      "  [-1.76050158e+00 -1.80025716e+00 -1.69451934e+00 -1.68903055e+00]\n",
      "  [-1.45228506e+00 -1.44493790e+00 -1.45333446e+00 -1.40927839e+00]\n",
      "  [-1.15794176e+00 -1.19359457e+00 -1.25968938e+00 -1.16510563e+00]\n",
      "  [-1.05506426e+01 -9.20046368e-01 -1.10381093e+00 -9.74619358e-01]\n",
      "  [-1.31858431e+01 -8.45089148e-01 -9.26309434e-01 -8.64827525e-01]]]\n"
     ]
    }
   ],
   "source": [
    "# Train Epsilon-Greedy Bandit\n",
    "bandit_agent.train()\n",
    "print(\"Epsilon-Greedy Bandit Q-Table:\")\n",
    "print(bandit_agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Armed Bandit Q-values (State-Action Values):\n",
      "[-9.73 -1.   -9.82 -9.82]\n"
     ]
    }
   ],
   "source": [
    "# Train Multi-Armed Bandit\n",
    "multi_arm_bandit_agent.train()\n",
    "print(\"Multi-Armed Bandit Q-values (State-Action Values):\")\n",
    "print(multi_arm_bandit_agent.q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Q-Table:\n",
      "[[[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [-19.           0.         -52.17031    -10.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [-90.99314404   0.           0.           0.        ]\n",
      "  [-10.         -97.97244404 -40.951      -99.9044995 ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [-10.         -46.8559     -96.56631618 -19.        ]\n",
      "  [-11.05882353 -87.08129523 -80.81620781 -80.67630443]\n",
      "  [100.          57.91200627 -90.99357883  61.93665533]\n",
      "  [-82.89382963  70.19        89.         -82.89382963]\n",
      "  [-90.99314404  62.171        0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [-90.99382963   0.           0.           0.        ]\n",
      "  [-90.99444667 -90.99153585 -82.89444667  74.86775   ]\n",
      "  [ 82.12709784  67.84598462  -7.34543603  65.35253825]\n",
      "  [ 71.73312159  62.171       77.01937129  41.10660178]\n",
      "  [ 54.9539      -8.19344764  70.19         0.        ]\n",
      "  [-10.         -79.41088679 -40.951      -99.91404955]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [  0.         -75.60595162   0.           0.        ]\n",
      "  [-19.         -10.           0.         -99.66186081]\n",
      "  [-61.93459779 -10.75       -76.34323675 -83.26825738]\n",
      "  [ 70.05974146  26.61643499 -90.99540588  46.02961921]\n",
      "  [ 62.06684008   0.           0.          24.51916557]\n",
      "  [ -7.9927196   28.35462841   0.         -82.89444667]\n",
      "  [-90.99382963   0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [-69.04535646   0.           0.         -82.8955018 ]\n",
      "  [-90.995002    29.01319348 -63.14082081  42.612659  ]\n",
      "  [-90.99635646  37.64428703  -3.75597636   0.        ]\n",
      "  [ 57.41418429  33.16549373  21.08006394  37.29900785]\n",
      "  [ 57.1200404    0.          48.45851    -69.0445018 ]\n",
      "  [-75.605002     0.          32.61625379   0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [-10.6        -77.22234162 -67.14689907 -85.51100716]\n",
      "  [ 23.81553484  12.83055067 -90.99688879  32.92220559]\n",
      "  [ 23.70662288  10.58452163  26.92756613  39.34664782]\n",
      "  [ 50.04176026 -90.99624251  43.11598677  35.78259669]\n",
      "  [ 50.76456326 -82.89595162   0.          32.61625379]\n",
      "  [  0.           0.          37.3513931    0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[-10.         -71.75704635 -99.89388834   0.        ]\n",
      "  [-90.96300115 -41.37810834 -90.99704873  23.8008146 ]\n",
      "  [ 24.64475156  -1.3178853    5.69535972  17.60234549]\n",
      "  [ 34.74893587 -39.74911485 -31.17092138 -80.70362163]\n",
      "  [-68.812976   -11.05882353 -85.99817582 -88.88632923]\n",
      "  [ 44.22669846   0.           0.         -90.9955018 ]\n",
      "  [-10.         -99.72610726 -99.21448328 -96.90968456]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[-10.         -99.03022627 -99.9822388  -88.52679384]\n",
      "  [-82.86670104 -75.60382963 -90.99672081   0.        ]\n",
      "  [-63.11982506 -19.84321083 -79.23853983  14.41174348]\n",
      "  [-10.87856956  15.40468788 -18.16061502 -82.75444217]\n",
      "  [-90.83826907   0.           0.          33.92362575]\n",
      "  [ 38.80402861   0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[-90.99238227   0.           0.           0.        ]\n",
      "  [-69.04344667   0.         -82.89314404   0.        ]\n",
      "  [ -4.14443819  10.38322208   0.           0.        ]\n",
      "  [-14.81876156  24.51916557  -7.76968845  25.57813686]\n",
      "  [ 29.53126318   0.          24.51916557   0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# Train Monte Carlo Agent\n",
    "monte_carlo_agent.train()\n",
    "print(\"Monte Carlo Q-Table:\")\n",
    "print(monte_carlo_agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, episodes=100):\n",
    "    total_rewards = 0\n",
    "    success_count = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        while state != env.goal:\n",
    "            if isinstance(agent, EpsilonGreedyBandit) or isinstance(agent, MonteCarlo):\n",
    "                action = np.argmax(agent.q_table[state[0], state[1], :])  # These agents require state information\n",
    "            elif isinstance(agent, MultiArmedBandit):\n",
    "                action = agent.select_action()  # No state parameter needed\n",
    "            else:  # For MDP agent\n",
    "                action = agent.policy_table[state[0], state[1]]\n",
    "\n",
    "            next_state, reward = env.step(state, action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if next_state == env.goal:\n",
    "                success_count += 1\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards += episode_reward\n",
    "        total_steps += steps\n",
    "\n",
    "    avg_reward = total_rewards / episodes\n",
    "    success_rate = success_count / episodes\n",
    "    avg_steps = total_steps / success_count if success_count > 0 else float('inf')\n",
    "\n",
    "    return avg_reward, success_rate, avg_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment and Agents\n",
    "env = GridWorld(GRID_SIZE, OBSTACLE_DENSITY)\n",
    "mdp_agent = MDP(env)\n",
    "bandit_agent = EpsilonGreedyBandit(env)\n",
    "multi_arm_bandit_agent = MultiArmedBandit(env)\n",
    "monte_carlo_agent = MonteCarlo(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MDP Value Iteration\n",
    "mdp_agent.value_iteration()\n",
    "mdp_avg_reward, mdp_success_rate, mdp_avg_steps = evaluate_agent(mdp_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Epsilon-Greedy Bandit\n",
    "bandit_agent.train()\n",
    "bandit_avg_reward, bandit_success_rate, bandit_avg_steps = evaluate_agent(bandit_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Multi-Armed Bandit\n",
    "multi_arm_bandit_agent.train()\n",
    "mab_avg_reward, mab_success_rate, mab_avg_steps = evaluate_agent(multi_arm_bandit_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Monte Carlo\n",
    "monte_carlo_agent.train()\n",
    "mc_avg_reward, mc_success_rate, mc_avg_steps = evaluate_agent(monte_carlo_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Results into a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Algorithm': ['MDP (Value Iteration)', 'Epsilon-Greedy Bandit', 'Monte Carlo'],\n",
    "    'Average Reward per Episode': [mdp_avg_reward, bandit_avg_reward, mc_avg_reward],\n",
    "    'Success Rate': [mdp_success_rate, bandit_success_rate, mc_success_rate],\n",
    "    'Average Steps to Goal': [mdp_avg_steps, bandit_avg_steps, mc_avg_steps]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Table:\n",
      "               Algorithm  Average Reward per Episode  Success Rate  \\\n",
      "0  MDP (Value Iteration)                        97.0           1.0   \n",
      "1  Epsilon-Greedy Bandit                        97.0           1.0   \n",
      "2            Monte Carlo                        97.0           1.0   \n",
      "\n",
      "   Average Steps to Goal  \n",
      "0                    4.0  \n",
      "1                    4.0  \n",
      "2                    4.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"Comparison Table:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there seems to be some issue with Multi Arm Bandit algorithm's training, owing to exceptionally long time to train.  Otherwise seems to be working fine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
